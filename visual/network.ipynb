{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f9d8d6",
   "metadata": {},
   "source": [
    "# Network X Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32d7e5",
   "metadata": {},
   "source": [
    "## DTM 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fade3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# DTM을 편리하게 만들어주기 위해 Scikit-Learn에서 제공하는 CountVectorizer를 import 한다.\n",
    "\n",
    "\n",
    "def makeDTM(category):\n",
    "    # 타이틀 리스트를 불러와서 title_list 변수에 저장한다.\n",
    "    t_file_name = open('./Title_List.txt', 'r', encoding='utf-8')\n",
    "\n",
    "    title_list = []\n",
    "    for line in t_file_name.readlines():\n",
    "        # txt파일을 readlines로 불러오면 개행 문자도 함께 읽어오기 때문에 인덱싱으로 처리해준다.\n",
    "        title_list.append(line[:-1])\n",
    "\n",
    "    t_file_name.close()\n",
    "\n",
    "    # pandas의 read_csv 함수를 이용하여 csv 파일을 불러온다.\n",
    "    dataset = pd.read_csv(f'./{category}_crwl.csv')\n",
    "\n",
    "    # 각 형태소별로 분류(Tagging)해주는 Okt 객체를 불러온다.\n",
    "    tagger = Okt()\n",
    "\n",
    "    for title in title_list:        # title_list에 대해 반복문을 실행\n",
    "        # 각 타이틀에 대한 6770개 문서의 DTM을 표현하기 위해\n",
    "        # CountVectorizer 객체를 선언\n",
    "        cv = CountVectorizer()\n",
    "        \n",
    "        # 각 문서들의 말뭉치(corpus)를 저장할 리스트 선언\n",
    "        corpus = []\n",
    "\n",
    "        # 각 타이틀에 대한 문서들의 말 뭉치를 저장한다. (데이터가 많으면 이 부분에서 장시간이 소요될 수 있다.)\n",
    "        for doc_num in range(len(dataset)):\n",
    "            # 각 말뭉치에서 명사 리스트를 만든다.\n",
    "            noun_list = tagger.nouns(dataset[title].loc[doc_num])\n",
    "            \n",
    "            # 이를 문자열로 저장해야하기 때문에 join함수로 공백으로 구분해 corpus에 append한다.\n",
    "            corpus.append(' '.join(noun_list))\n",
    "\n",
    "        # CountVectorizer의 fit_transform 함수를 통해 DTM을 한번에 생성할 수 있다.\n",
    "        DTM_Array = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "        # feature_names 함수를 사용하면 DTM의 각 열(column)이 어떤 단어에 해당하는지 알 수 있다.\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        # 추출해낸 데이터를 DataFrame 형식으로 변환한다.\n",
    "        DTM_DataFrmae = pd.DataFrame(DTM_Array, columns=feature_names)\n",
    "\n",
    "        # 최종적으로 DTM을 csv 파일로 저장한다.\n",
    "        DTM_DataFrmae.to_csv('DTM.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53c0cd",
   "metadata": {},
   "source": [
    "## StopWord delete and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def NLP_DTM(category):\n",
    "    # 타이틀 리스트를 불러와서 title_list 변수에 저장한다.\n",
    "    t_file_name = open('./Title_List.txt', 'r', encoding='utf-8')\n",
    "\n",
    "    title_list = []\n",
    "    for line in t_file_name.readlines():\n",
    "        # txt파일을 readlines로 불러오면 개행 문자도 함께 읽어오기 때문에 인덱싱으로 처리해준다.\n",
    "        title_list.append(line[:-1])\n",
    "\n",
    "    t_file_name.close()\n",
    "\n",
    "    # 불용어 파일을 불러와서 stop_words_list 변수에 저장한다.\n",
    "    s_file_name = open('./koreanStopwords.txt', 'r', encoding='utf-8')\n",
    "\n",
    "    stop_words_list = []\n",
    "    for line in s_file_name.readlines():\n",
    "        # 위에서는 인덱싱으로 처리했지만 rstrip 함수를 사용하여 공백을 제거할 수도 있다.\n",
    "        stop_words_list.append(line.rstrip())\n",
    "\n",
    "    s_file_name.close()\n",
    "\n",
    "    # pandas의 read_csv 함수를 이용하여 csv 파일을 불러온다.\n",
    "    dataset = pd.read_csv(f'./{category}_crwl.csv')\n",
    "\n",
    "    # 각 형태소별로 분류(Tagging)해주는 Okt 객체를 불러온다.\n",
    "    tagger = Okt()\n",
    "\n",
    "    for title in tqdm(title_list, desc='타이틀 리스트 진행도'):  # title_list에 대해 반복문을 실행\n",
    "        # 각 타이틀에 대한 6770개 문서의 DTM을 표현하기 위해\n",
    "        # CountVectorizer 객체를 선언\n",
    "        cv = CountVectorizer()\n",
    "\n",
    "        # 각 문서들의 말뭉치(corpus)를 저장할 리스트 선언\n",
    "        corpus = []\n",
    "\n",
    "        # 각 타이틀에 대한 문서들의 말 뭉치를 저장한다. (데이터가 많으면 이 부분에서 장시간이 소요될 수 있다.)\n",
    "        for doc_num in tqdm(range(len(dataset)), desc='문서 진행도'):\n",
    "            # 각 말뭉치에서 명사 리스트를 만든다.\n",
    "            noun_list = tagger.nouns(dataset[title].loc[doc_num])\n",
    "\n",
    "            # 이를 문자열로 저장해야하기 때문에 join함수로 공백으로 구분해 corpus에 append한다.\n",
    "            corpus.append(' '.join(noun_list))\n",
    "\n",
    "        # CountVectorizer의 fit_transform 함수를 통해 DTM을 한번에 생성할 수 있다.\n",
    "        DTM_Array = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "        # feature_names 함수를 사용하면 DTM의 각 열(column)이 어떤 단어에 해당하는지 알 수 있다.\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        # 추출해낸 데이터를 DataFrame 형식으로 변환한다.\n",
    "        DTM_DataFrmae = pd.DataFrame(DTM_Array, columns=feature_names)\n",
    "\n",
    "        # 열 제거는 drop 함수를 사용하며 axis 속성을 columns로 준 후 inplace를 True로 한다.\n",
    "        del_col = set(DTM_DataFrmae.columns).intersection(set(stop_words_list))\n",
    "        DTM_DataFrmae.drop(del_col, axis='columns', inplace=True)\n",
    "\n",
    "        # 최종적으로 DTM을 csv 파일로 저장한다.\n",
    "        DTM_DataFrmae.to_csv('DTM.csv', encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b108e",
   "metadata": {},
   "source": [
    "## co-occurence network anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c205ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def networkX():\n",
    "    # 단어쌍의 빈도를 체크하기위해 DTM을 불러온다.\n",
    "    dataset = pd.read_csv('DTM.csv')\n",
    "\n",
    "    # 단어들의 목록을 가져온다.\n",
    "    # 이때 0번째 인덱스에는 빈 칸이 들어오므로 인덱싱을 통해 없애준다.\n",
    "    column_list = dataset.columns[1:]\n",
    "    word_length = len(column_list)\n",
    "\n",
    "    # 각 단어쌍의 빈도수를 저장할 dictionary 생성\n",
    "    count_dict = {}\n",
    "\n",
    "    for doc_number in tqdm(range(len(dataset)), desc='단어쌍 만들기 진행중'):\n",
    "        tmp = dataset.loc[doc_number]           # 현재 문서의 단어 출현 빈도 데이터를 가져온다.\n",
    "        for i, word1 in enumerate(column_list):\n",
    "            if tmp[word1]:              # 현재 문서에 첫번째 단어가 존재할 경우\n",
    "                for j in range(i + 1, word_length):\n",
    "                    if tmp[column_list[j]]:              # 현재 문서에 두번째 단어가 존재할 경우\n",
    "                        count_dict[column_list[i], column_list[j]] = count_dict.get((column_list[i], column_list[j]), 0) + max(tmp[word1], tmp[column_list[j]])\n",
    "\n",
    "    # count_list에 word1, word2, frequency 형태로 저장할 것이다.\n",
    "    count_list = []\n",
    "\n",
    "    for words in count_dict:\n",
    "        count_list.append([words[0], words[1], count_dict[words]])\n",
    "\n",
    "    # 단어쌍 동시 출현 빈도를 DataFrame 형식으로 만든다.\n",
    "    df = pd.DataFrame(count_list, columns=[\"word1\", \"word2\", \"freq\"])\n",
    "    df = df.sort_values(by=['freq'], ascending=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # 이 작업이 오래 걸리기 때문에 csv파일로 저장 후 사용하는 것을 추천한다.\n",
    "    df.to_csv('networkx.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6476eb",
   "metadata": {},
   "source": [
    "## Network X Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "def show(show_num,category,iteration):\n",
    "    # 단어쌍 동시출현 빈도수를 담았던 networkx.csv파일을 불러온다.\n",
    "    dataset = pd.read_csv('./networkx.csv')\n",
    "\n",
    "    # 중심성 척도 계산을 위한 Graph를 만든다\n",
    "    G_centrality = nx.Graph()\n",
    "\n",
    "    # 빈도수가 20000 이상인 단어쌍에 대해서만 edge(간선)을 표현한다.\n",
    "    for ind in range(len(np.where(dataset['freq'])[0][:show_num])):\n",
    "        G_centrality.add_edge(dataset['word1'][ind], dataset['word2'][ind], weight=int(dataset['freq'][ind]))\n",
    "\n",
    "    dgr = nx.degree_centrality(G_centrality)        # 연결 중심성\n",
    "#     btw = nx.betweenness_centrality(G_centrality)   # 매개 중심성\n",
    "#     cls2 = nx.closeness_centrality(G_centrality)     # 근접 중심성\n",
    "#     egv = nx.eigenvector_centrality(G_centrality)   # 고유벡터 중심성\n",
    "    pgr = nx.pagerank(G_centrality)                 # 페이지 랭크\n",
    "\n",
    "    # 중심성이 큰 순서대로 정렬한다.\n",
    "    sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#     sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#     sorted_cls = sorted(cls2.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#     sorted_egv = sorted(egv.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_pgr = sorted(pgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    # 단어 네트워크를 그려줄 Graph 선언\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 페이지 랭크에 따라 두 노드 사이의 연관성을 결정한다. (단어쌍의 연관성)\n",
    "    # 연결 중심성으로 계산한 척도에 따라 노드의 크기가 결정된다. (단어의 등장 빈도수)\n",
    "    for i in range(len(sorted_pgr)):\n",
    "        G.add_node(sorted_pgr[i][0], nodesize=sorted_dgr[i][1])\n",
    "\n",
    "    for ind in range(len(np.where(dataset['freq'])[0][:show_num])):\n",
    "        G.add_weighted_edges_from([(dataset['word1'][ind], dataset['word2'][ind], int(dataset['freq'][ind]))])\n",
    "\n",
    "\n",
    "    # 노드 크기 조정\n",
    "    sizes = [G.nodes[node]['nodesize'] * 1000 for node in G]\n",
    "\n",
    "    options = {\n",
    "        'edge_color': '#bfc3f5',\n",
    "        'width': 1.5,\n",
    "        'with_labels': True,\n",
    "        'font_weight': 'bold',\n",
    "        'font_size':11,\n",
    "    }\n",
    "\n",
    "    # 폰트 설정을 위한 font_manager import\n",
    "    import matplotlib.font_manager as fm\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 폰트 설정\n",
    "\n",
    "    font_fname = '/usr/share/fonts/nanum/NanumMyeongjo.ttf'      \n",
    "    fontprop = fm.FontProperties(fname=font_fname, size=100).get_name()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    nx.draw(G, node_size=sizes, pos=nx.spring_layout(G, k=4, iterations=iteration), node_color=\"#edabc5\", **options, font_family=fontprop)  # font_family로 폰트 등록\n",
    "#     nx.draw(G, node_size=0,pos=nx.spring_layout(G, k=2.5, iterations=100), **options, font_family=fontprop)  # font_family로 폰트 등록\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#bfc3f5\")\n",
    "\n",
    "\n",
    "    fig.set_facecolor('#ffffff')\n",
    "    \n",
    "    plt.savefig(f'{category}_network.png', dpi=600)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
