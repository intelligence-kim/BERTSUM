{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85803891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from BERT.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "import sentencepiece as spm\n",
    "import wget\n",
    "import import_ipynb\n",
    "import BERT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c8d1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file = \"/home/studio/바탕화면/web-crawler/kowiki/vocab_32000/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a7ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BERT.BERT(self.config)\n",
    "        # classfier\n",
    "        self.projection_cls = nn.Linear(self.config.d_model, self.config.n_output, bias=False)\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        # (bs, n_output)\n",
    "        logits_cls = self.projection_cls(outputs_cls)\n",
    "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return logits_cls, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e100feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentences = []\n",
    "        self.segments = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        with open(infile, \"r\") as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "\n",
    "        with open(infile, \"r\") as f:\n",
    "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=\"Loading Dataset\", unit=\" lines\")):\n",
    "                data = json.loads(line)\n",
    "                self.labels.append(data[\"label\"])\n",
    "                sentence = [vocab.piece_to_id(\"[CLS]\")] + [vocab.piece_to_id(p) for p in data[\"doc\"]] + [vocab.piece_to_id(\"[SEP]\")]\n",
    "                self.sentences.append(sentence)\n",
    "                self.segments.append([0] * len(sentence))\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentences)\n",
    "        assert len(self.labels) == len(self.segments)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor(self.segments[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888e5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collate_fn(inputs):\n",
    "    labels, inputs, segments = list(zip(*inputs))\n",
    "\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels, dim=0),\n",
    "        inputs,\n",
    "        segments,\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510e8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 100%|███████████| 149995/149995 [00:02<00:00, 54587.82 lines/s]\n",
      "Loading Dataset: 100%|█████████████| 49997/49997 [00:00<00:00, 51444.83 lines/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_dataset = DataSet(vocab, f\"ratings_train.json\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collate_fn)\n",
    "test_dataset = DataSet(vocab, f\"ratings_test.json\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1327675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(config, model, data_loader):\n",
    "    matchs = []\n",
    "    model.eval()\n",
    "\n",
    "    n_word_total = 0\n",
    "    n_correct_total = 0\n",
    "    with tqdm(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
    "        for i, value in enumerate(data_loader):\n",
    "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            outputs = model(inputs, segments)\n",
    "            logits_cls = outputs[0]\n",
    "            _, indices = logits_cls.max(1)\n",
    "\n",
    "            match = torch.eq(indices, labels).detach()\n",
    "            matchs.extend(match.cpu())\n",
    "            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Acc: {accuracy:.3f}\")\n",
    "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37495246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
    "        for i, value in enumerate(train_loader):\n",
    "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, segments)\n",
    "            logits_cls = outputs[0]\n",
    "\n",
    "            loss_cls = criterion_cls(logits_cls, labels)\n",
    "            loss = loss_cls\n",
    "\n",
    "            loss_val = loss_cls.item()\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b705dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90c812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 32007, 'n_enc_seq': 512, 'n_seg_type': 2, 'n_layer': 6, 'd_model': 512, 'i_pad': 0, 'd_ff': 1024, 'n_head': 6, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"n_enc_vocab\": len(vocab), # vocab 크기\n",
    "    \"n_enc_seq\": 512,          # 글자 최대 길이 \n",
    "    \"n_seg_type\": 2,           # Segment Embedding Type  \n",
    "    \"n_layer\":6,             # layer 캣수\n",
    "    \"d_model\": 512,            # hidden layer \n",
    "    \"i_pad\": 0,                # padding 값\n",
    "    \"d_ff\": 1024,              # feedforward layer에 들어갈 차원의 크기\n",
    "    \"n_head\": 6,              # attention 개수\n",
    "    \"d_head\": 64,              # attention 차원 \n",
    "    \"dropout\": 0.1,            # dropout\n",
    "    \"layer_norm_epsilon\": 1e-12 # 정규화\n",
    "})\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4576096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 32007, 'n_enc_seq': 512, 'n_seg_type': 2, 'n_layer': 6, 'd_model': 512, 'i_pad': 0, 'd_ff': 1024, 'n_head': 6, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cuda'), 'n_output': 2}\n"
     ]
    }
   ],
   "source": [
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.n_output = 2\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301e2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.to(config.device)\n",
    "\n",
    "    criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_epoch, best_loss, best_score = 0, 0, 0\n",
    "    losses, scores = [], []\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader)\n",
    "        score = eval_epoch(config, model, test_loader)\n",
    "\n",
    "        losses.append(loss)\n",
    "        scores.append(score)\n",
    "\n",
    "        if best_score < score:\n",
    "            best_epoch, best_loss, best_score = epoch, loss, score\n",
    "    print(f\">>>> epoch={best_epoch}, loss={best_loss:.5f}, socre={best_score:.5f}\")\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4e71e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 4.929350027394117)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BinaryClassification(config)\n",
    "save_pretrain = \"save_bert_pretrain.pth\"\n",
    "model.bert.load(save_pretrain)\n",
    "# losses, scores = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa0a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train(0): 100%|████████| 9375/9375 [03:02<00:00, 51.45it/s, Loss: 0.557 (0.443)]\n",
      "Valid: 100%|████████████████████| 3125/3125 [02:17<00:00, 22.77it/s, Acc: 0.824]\n",
      "Train(1): 100%|████████| 9375/9375 [03:02<00:00, 51.48it/s, Loss: 0.479 (0.347)]\n",
      "Valid: 100%|████████████████████| 3125/3125 [02:16<00:00, 22.94it/s, Acc: 0.836]\n",
      "Train(2): 100%|████████| 9375/9375 [03:01<00:00, 51.55it/s, Loss: 0.231 (0.298)]\n",
      "Valid: 100%|████████████████████| 3125/3125 [02:16<00:00, 22.84it/s, Acc: 0.849]\n",
      "Train(3): 100%|████████| 9375/9375 [03:01<00:00, 51.62it/s, Loss: 0.079 (0.255)]\n",
      "Valid: 100%|████████████████████| 3125/3125 [02:16<00:00, 22.92it/s, Acc: 0.848]\n",
      "Train(4): 100%|████████| 9375/9375 [03:03<00:00, 51.01it/s, Loss: 0.084 (0.215)]\n",
      "Valid:  74%|██████████████▊     | 2311/3125 [01:19<00:50, 16.27it/s, Acc: 0.850]"
     ]
    }
   ],
   "source": [
    "losses, scores = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24857fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fe7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
